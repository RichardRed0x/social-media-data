#!/usr/bin/env python3

# tsu - time series utility

import os
import csv
import json
from datetime import datetime
from itertools import chain
from urllib.parse import urlparse, urlunparse
import time

class Config:
    data_dir            = "data/data"
    export_filename     = "export.csv"
    profile_filename    = "profile.json"
    graph_list_file     = "graph.list"

class Const:
    int_exts            = (".csv", ".tsi")
    str_exts            = (".tss", )

# Define and catch our own exception to avoid intercepting language and library
# exceptions.
class TsuError(Exception):
    pass

class ValidationError(TsuError):
    pass

def parse(entry, valtype):
    if len(entry) != 2:
        raise ValidationError(
            "entry must have exactly 2 fields: " + str(entry))
    s1, s2 = entry
    try:
        tsi = int(s1)
    except ValueError as e:
        raise ValidationError(
            "timestamp is not an integer: '{}'".format(s1)) from e
    try:
        dt = datetime.utcfromtimestamp(tsi)
    except Exception as e:
        raise ValidationError(
            "cannot parse datetime from timestamp: '{}'".format(tsi)) from e
    if valtype == int:
        try:
            val = int(s2)
        except ValueError as e:
            raise ValidationError(
                "value is not an integer: '{}'".format(s2)) from e
    else:
        if len(s2) > 0:
            val = s2
        else:
            raise ValidationError("value is empty")
    return dt, val

def value_type(path):
    ext = os.path.splitext(path)[1]
    if ext in Const.int_exts:
        return int
    elif ext in Const.str_exts:
        return str
    else:
        return None

def csv_iter(filename):
    """Generator yielding rows of the csv file."""
    with open(filename, newline="") as f:
        yield from csv.reader(f)

def write_csv(filename, rows):
    # overwrite existing file
    with open(filename, "w", newline="") as f:
        writer = csv.writer(f, quoting=csv.QUOTE_MINIMAL, lineterminator="\n")
        writer.writerows(rows)

def print_entry(dt, val, valtype):
    if valtype == int:
        print(dt, "{:>6,}".format(val))
    else:
        print(dt, val)

def validate(path, view=False):
    valtype = value_type(path)
    prev_dt = datetime.min

    # start counting from 1 to match file line number
    for line_num, row in enumerate(csv_iter(path), start=1):
        try:
            dt, val = parse(row, valtype)
            if dt <= prev_dt:
                raise ValidationError(
                    "timestamp '{}' must be greater than '{}'".format(dt, prev_dt))
            prev_dt = dt
            if view:
                print_entry(dt, val, valtype)
        except ValidationError as e:
            print("err {}:{}: {}".format(path, line_num, e))

def cmd_view(args):
    path = args.file
    if not os.path.isfile(path):
        raise TsuError("expected a file but got a directory: " + path)
    # try to handle the file regardless of its type
    validate(args.file, view=True)

def cmd_validate(args):
    path = args.path
    if os.path.isfile(path):
        # try to handle the file regardless of its type
        validate(path)
    elif os.path.isdir(path):
        for curdir, dirs, files in os.walk(path):
            for fname in files:
                # only validate files of known type
                if value_type(fname) is not None:
                    validate(os.path.join(curdir, fname))

def load_list(path):
    with open(path) as f:
        return [line.rstrip("\n") for line in f]

def strip_prefix(s, p):
    if s.startswith(p):
        return s[len(p):]
    else:
        return s

def load_profile(dirpath):
    ppath = os.path.join(dirpath, Config.profile_filename)
    if os.path.exists(ppath):
        return load_json(ppath)
    else:
        return None

def metric_attrs(path):
    prefix = Config.data_dir + os.sep
    if not path.startswith(prefix):
        # didn't consider this case thoroughly
        return None
    innerpath = strip_prefix(path, prefix)
    parts = innerpath.split(os.sep)
    valid = len(parts) >= 3
    if not valid:
        return None
    # platform is the first path component under Config.data_dir
    # account is 1+ path components under platform, except the last one
    platform, *account, metric_ext = parts
    account_str = "/".join(account)
    # metric name is the file name without extension
    metric = os.path.splitext(metric_ext)[0]
    return platform, account_str, metric

def export_append_rows(rows, fpath, graph):
    attrs = metric_attrs(fpath)
    if not attrs:
        print("WARNING: skipping unexpected file:", fpath)
        return

    platform, account, metric = attrs
    graph_int = int(graph)
    tags = ""
    profile = load_profile(os.path.dirname(fpath))
    if profile and ("tags" in profile):
        tags = " ".join(profile["tags"])
    for ts, val in csv_iter(fpath):
        rows.append((ts, platform, account, metric, val, graph_int, tags))

def export_csv(path, outname, graph_only):
    rows = []
    if os.path.isfile(path):
        # try to handle the file regardless of its type
        export_append_rows(rows, path)
    else:
        graph_paths = []
        if os.path.isfile(Config.graph_list_file):
            graph_paths = load_list(Config.graph_list_file)
            print("loaded {} paths from {}".format(
                len(graph_paths), Config.graph_list_file))
        if graph_only:
            export_paths = graph_paths
            print("exporting {} paths".format(len(export_paths)))
        else:
            print("exporting paths:", path)
            export_paths = [path]
        for ep in export_paths:
            for curdir, dirs, files in os.walk(ep):
                for fname in files:
                    # only export files with int values
                    if value_type(fname) == int:
                        fpath = os.path.join(curdir, fname)
                        # set `graph` column to True if the file is whitelisted
                        graph = any(map(fpath.startswith, graph_paths))
                        export_append_rows(rows, fpath, graph)

    # sort by timestamp, in-place
    rows.sort()
    print("found {} data points".format(len(rows)))

    header = ("timestamp", "platform", "account", "metric", "value", "graph",
              "tags")

    # overwrite existing file
    write_csv(outname, chain([header], rows))
    print("file saved:", outname)

def cmd_export_csv(args):
    export_csv(args.path, args.output, args.graph_only)

def load_json(path):
    with open(path) as f:
        return json.load(f)

def replace_url(url):
    # replace some URLs for easier data point collection
    pu = urlparse(url)
    if pu.netloc == "twitter.com":
        newpu = pu._replace(netloc="nitter.net")
        newurl = urlunparse(newpu)
        return newurl
    return url

def make_hint(dirpath, default_hint):
    profile = load_profile(dirpath)
    hint = ""
    if profile:
        if "name" in profile:
            hint += '"' + profile["name"] + '"'
        if "url" in profile:
            if hint:
                hint += " "
            hint += replace_url(profile["url"])

    return hint if hint else default_hint

def optional_input(prompt):
    """Read a string from standard input, cancel on double blank entry.

    Wrap standard input() and return (True, string) on success or
    (False, None) if user cancels.
    """
    try:
        s = input(prompt)
        if s == "":
            s = input("enter blank again to skip or a value to continue: ")
            if s == "":
                return (False, None)
        return (True, s)
    except EOFError:
        print("(got EOF)")
        return (False, None)

def confirmed_input(prompt, confirm_prompt):
    """Read inputs until two subsequent inputs match, cancel on blank input.

    If user cancels input, return what optional_input returned.
    """
    # set up the loop
    ok, prev = optional_input(prompt)
    if not ok:
        return (ok, prev)

    # keep collecting inputs until (a) two inputs match or (b) input is canceled
    while True:
        ok, cur = optional_input(confirm_prompt)
        if (not ok) or (cur == prev):
            return (ok, cur)
        prev = cur

def make_prompt_prefix(path):
    # treat file's parent dir as "account" and filename w/o ext as "metric"
    dirpath, filename = os.path.split(path)
    absdirpath = os.path.abspath(dirpath)
    acc = os.path.basename(absdirpath)
    metric = os.path.splitext(filename)[0]
    return acc + "/" + metric

def entry_file(path):
    dirpath, filename = os.path.split(path)

    prompt_prefix = make_prompt_prefix(path)

    hint = make_hint(dirpath, filename)
    print("{}: capture the value for {}".format(prompt_prefix, hint))

    valtype = value_type(filename)
    rows = list(csv_iter(path)) # read all to get the length
    had_values = len(rows) > 0
    if had_values:
        last_dt, last_val = parse(rows[-1], valtype)
        # extra output for a sanity check
        print("{}: last record is: time {}, value {}".format(
            prompt_prefix, last_dt, last_val))

    delta = ""
    if valtype == int:
        while True: # entry loop for int value
            ok, s = confirmed_input(
                "{}: enter integer value: ".format(prompt_prefix),
                "{}: confirm integer value: ".format(prompt_prefix))
            if not ok:
                print("{}: skipping".format(prompt_prefix))
                return
            try:
                val = int(s)
                break # entry loop
            except ValueError as e:
                print("{}: expected an integer but got: {}".format(prompt_prefix, s))

        delta = " ({:+})".format(val - last_val) if had_values else ""
    else:
        ok, s = confirmed_input(
            "{}: enter string value: ".format(prompt_prefix),
            "{}: confirm string value: ".format(prompt_prefix))
        if not ok:
            print("{}: skipping".format(prompt_prefix))
            return
        val = s

    now = int(time.time())
    rows.append((now, val))
    write_csv(path, rows)
    print("{}: saved: time {}, value {}{}".format(
        prompt_prefix, datetime.utcfromtimestamp(now), val, delta))

def cmd_entry(args):
    """Enter data manually in an interactive session.

    You will be prompted to enter data points one by one for each
    applicable file. After collecting the input value, UTC timestamp
    will be generated and appended to the end of file together with the
    new value. Make sure your system clock is accurate.

    To protect from errors, you will be prompted to enter each value
    twice.

    To skip entering current value, enter a blank line twice or hit
    Ctrl-D (Ctl-Z+Return on Windows).

    To quit the data entry session, hit Ctrl-C.
    """
    print("Interactive data entry mode. Make sure your system clock is accurate.")
    print("Your system UTC time is:",
          format(datetime.utcfromtimestamp(int(time.time()))))
    path = args.path
    if os.path.isfile(path):
        # try to handle the file regardless of its type
        entry_file(path)
    elif os.path.isdir(path):
        for curdir, dirs, files in os.walk(path):
            for fname in files:
                # only prompt entry for int values
                if value_type(fname) == int:
                    entry_file(os.path.join(curdir, fname))

def make_arg_parser():
    import argparse

    parser = argparse.ArgumentParser(
        description=("time series utility. The following file types are"
                     "supported: " + ", ".join(Const.int_exts + Const.str_exts)))
    subparsers = parser.add_subparsers(dest="command", title="commands")

    validate = subparsers.add_parser(
        "validate", aliases=["val"],
        help="validate time series files")
    validate.add_argument(
        "path", nargs="?",
        default=Config.data_dir,
        help="path to search time series files")
    validate.set_defaults(func=cmd_validate)

    view = subparsers.add_parser(
        "view", aliases=["v"],
        help="view time series file")
    view.add_argument(
        "file",
        help="file to view")
    view.set_defaults(func=cmd_view)

    export = subparsers.add_parser(
        "export",
        help="export data into a single file",
        description=("Export data from arbitrary tree of time series files"
                     " into a single file (" + Config.export_filename +
                     " by default), overwriting it."))
    export.add_argument(
        "path", nargs="?",
        default=Config.data_dir,
        help="path to search time series files")
    export.add_argument(
        "-g", "--graph-only",
        action="store_true",
        help="export only paths listed in " + Config.graph_list_file)
    export.add_argument(
        "--output",
        default=Config.export_filename,
        help="file name to save")
    export.set_defaults(func=cmd_export_csv)

    entry = subparsers.add_parser(
        "entry", aliases=["e"],
        help="enter data manually",
        description=cmd_entry.__doc__)
    entry.add_argument(
        "path", nargs="?",
        default=Config.data_dir,
        help="file or directory path to enter data in; if directory, the"
             " program will prompt data entry for each time sieries file found"
             " in it")
    entry.set_defaults(func=cmd_entry)

    return parser

def main():
    parser = make_arg_parser()
    args = parser.parse_args()

    if args.command:
        try:
            args.func(args)
        except TsuError as e:
            print("error:", e)
        except KeyboardInterrupt:
            print("\naborting")
        except BrokenPipeError:
            # silence error when e.g. piping into `less` and quitting before
            # reading all
            pass
    else:
        parser.print_usage()

if __name__ == "__main__":
    main()
